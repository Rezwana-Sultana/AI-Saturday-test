{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# নিচের ব্লক টি রান করতে হবে ডিপেনড্যান্সিগুলো ইন্সটল করার জন্য "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this block to install dependencies [Remember to make the statement true]\n",
    "if 0 == 1:\n",
    "    !pip3 install pandas\n",
    "    !pip3 install tqdm\n",
    "    !pip3 install scikit-learn\n",
    "    !pip3 install gensim\n",
    "    !pip3 install spacy\n",
    "    !python3 -m spacy download en\n",
    "    !pip3 install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
   
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rezwana\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.utils import shuffle\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Warning: no model found for 'bn'\n",
      "\n",
      "    Only loading the 'bn' tokenizer.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "আমার োনার বাংলা"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nlp = spacy.load('bn')\n",
    "doc = nlp(u'আমার োনার বাংলা')\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model with shortcut link \"en\"\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "\n",
    "DATA_LIMIT = 1000\n",
    "\n",
    "# df stands for data frame\n",
    "df = pd.read_csv('./imdb_master.csv', encoding='latin1')\n",
    "\n",
    "df_neg = df[df['label'] == 'neg']\n",
    "df_pos = df[df['label'] == 'pos']\n",
    "\n",
    "df = pd.concat((df_pos[:DATA_LIMIT], df_neg[:DATA_LIMIT]))\n",
    "\n",
    "df_test=pd.concat((df_pos[1000:1500], df_neg[1000:1500]))\n",
    "\n",
    "#removing stop word, converting text to lower case, removing digits\n",
    "\n",
    "def process_text(input_string, return_string=False, stem=False):\n",
    "    text = nlp(u'' + input_string)\n",
    "    if stem == True:\n",
    "        text = [tok.lemma_ for tok in text if (tok.is_alpha and not tok.is_stop)]\n",
    "    else:\n",
    "        text = [tok.lower_ for tok in text if (tok.is_alpha and not tok.is_stop)]\n",
    "    if return_string == True:\n",
    "        return \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make this statement true to run from scratch [It takes time to process the text]\n",
    "\n",
    "#tqdm is for the progress bar\n",
    "#df.shape[0] is used for number of row count\n",
    "#df.shape[1] is used for number of column count\n",
    "#wordlist contains all the words in the dataframe except stop words and digits, all lowercase\n",
    "#wb for writing in binary\n",
    "\n",
    "if 1 == 0:\n",
    "    wordlist = []\n",
    "    \n",
    "    for i in tqdm(range(df.shape[0])): \n",
    "        \n",
    "        wordlist.append(process_text(df['review'].iloc[i]))\n",
    "        \n",
    "    with open('vocabulary.txt', 'wb') as vocabulary:\n",
    "        pickle.dump(wordlist, vocabulary) #writing wordlist to vocabulary\n",
    "    vocabulary.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load vocabulary\n",
    "wordlist = []\n",
    "with open('vocabulary.txt', 'rb') as vocabulary:\n",
    "    wordlist = pickle.load(vocabulary) # wordlist <-- vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens - 1088\n"
     ]
    }
   ],
   "source": [
    "# Keeping track of frequency of a single token\n",
    "frequency = defaultdict(int) #defaultdict(int, {})\n",
    "for text in wordlist:\n",
    "    for token in text:\n",
    "        frequency[token] += 1 #counting frequency of each token\n",
    "        \n",
    "# Apply Threshold to limit the vocabulary size, discarding the tokens which appeard number of times below the threshold limit \n",
    "FREQ_THRESHOLD = 35 # set a threshold to get the words who are frequency more than 35\n",
    "\n",
    "thresholded_wordlist =  [[token for token in text if frequency[token] > FREQ_THRESHOLD]\n",
    "          for text in wordlist] # thresholded_wordlist contains the words who have frq more than 35, contains duplicate value\n",
    "\n",
    "# Create Dictionary based on the word list\n",
    "dictionary = Dictionary(thresholded_wordlist) # unique words are in dictionary\n",
    "\n",
    "# Number of tokens\n",
    "print(\"Number of Tokens - {}\".format(len(dictionary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now dumping the words from the dataframe in lowercase format\n",
    "with open('dump_skipgram.txt','w', encoding='utf-8') as f:\n",
    "    for i in range(2000):\n",
    "        f.write(df['review'].iloc[i].lower())\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 4913.05it/s]\n"
     ]
    }
   ],
   "source": [
    "#storing the words from data frame to dump variable \n",
    "#tqdm for the progress bar\n",
    "dump=\"\"\n",
    "for i in tqdm(range(2000)):\n",
    "         dump+=(df['review'].iloc[i].lower())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking the whole dump sentence on spacy_obj\n",
    "spacy_obj=nlp(u''+dump) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "525604"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spacy_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot representation 1088*1088 dimention\n",
    "#one_hot=np.eye(1088)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#window size, m=2 \n",
    "m=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inversing the dictionary; \n",
    "#word2idx contains word to index\n",
    "#will return index for a given word id it exists\n",
    "\n",
    "word2idx = { dictionary[i]:i for i, word in enumerate(dictionary) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx['able']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx['i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will return the index if found on word2idx \n",
    "#else return -1\n",
    "\n",
    "def getIndex(word):\n",
    "    try:\n",
    "        idx=word2idx[word]\n",
    "    except KeyError:\n",
    "        idx=-1\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the index of left and right words of the center word according to the window size\n",
    "def input_generation(word):\n",
    "    \n",
    "    inp1=np.zeros(4)\n",
    "    index=getIndex(str(word.nbor(-2)))   \n",
    "    inp1[0]=index\n",
    "    index=getIndex(str(word.nbor(-1)))   \n",
    "    inp1[1]=index\n",
    "    index=getIndex(str(word.nbor(1)))   \n",
    "    inp1[2]=index\n",
    "    index=getIndex(str(word.nbor(2)))   \n",
    "    inp1[3]=index\n",
    "    return inp1      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 525602/525602 [00:07<00:00, 71938.14it/s]\n"
     ]
    }
   ],
   "source": [
    "len_s=len(spacy_obj)\n",
    "\n",
    "inp=np.zeros((len_s,4)) # for each word in the spacy_obj spaces are created according to the windowsize\n",
    "target=np.zeros(len_s) # target has 1*len_s dimention\n",
    "for i in tqdm(range(len_s-2)):\n",
    "    \n",
    "    token=spacy_obj[i]\n",
    "#     print(token)\n",
    "    token1=str(token) #need to convert to string to send to the getIndex function\n",
    "    index=getIndex(token1)\n",
    "    target[i]=index #storing the index of the token in target \n",
    "#     print(target[i])\n",
    "    inp[i,:]=input_generation(spacy_obj[i]) # creating the input window for the word in the index i\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp.shape\n",
    "inp=inp[np.where(target!=-1)[0]] # storing the indexs whose target value found in the dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_positive=target[np.where(target!=-1)[0]] # removing the indexs with -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_positive[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(291908,)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_positive.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rezwana\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# keras help: https://keras.io/getting-started/sequential-model-guide/\n",
    "# keras is taken to calculate mean value according to column\n",
    "# to implement neural network\n",
    "#The Sequential model is a linear stack of layers.\n",
    "\n",
    "# Some 2D layers, such as Dense, support the specification of their input shape\n",
    "#via the argument input_dim, and some 3D temporal layers support the arguments input_dim and input_length.\n",
    "\n",
    "#Embedding is for to compress the input feature space into a smaller one\n",
    "#Labmda is for \n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow=Sequential()\n",
    "cbow.add(Embedding(input_dim=1089, output_dim=50, input_length=4))\n",
    "cbow.add(Lambda(lambda x:K.mean(x, axis=1), output_shape=(50,))) \n",
    "cbow.add(Dense(1089, activation='softmax')) #softmax is used for multiclass classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "cbow.compile(loss='sparse_categorical_crossentropy', optimizer='adadelta')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=inp+1\n",
    "train_y=target_positive+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow.fit(train_x, train_y, batch_size=128, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow.predict_classes(train_x[180].reshape((1, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y[180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
