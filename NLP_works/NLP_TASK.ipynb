{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this block to install dependencies [Remember to make the statement true]\n",
    "if 0 == 1:\n",
    "    !pip3 install pandas\n",
    "    !pip3 install tqdm\n",
    "    !pip3 install scikit-learn\n",
    "    !pip3 install gensim\n",
    "    !pip3 install spacy\n",
    "    !python3 -m spacy download en\n",
    "    !pip3 install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Warning: no model found for 'en'\n",
      "\n",
      "    Only loading the 'en' tokenizer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.utils import shuffle\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "DATA_LIMIT = 1000\n",
    "\n",
    "df = pd.read_csv('./imdb_master.csv', encoding='latin1')\n",
    "df_neg = df[df['label'] == 'neg']\n",
    "df_pos = df[df['label'] == 'pos']\n",
    "df = pd.concat((df_pos[:DATA_LIMIT], df_neg[:DATA_LIMIT]))\n",
    "\n",
    "def process_text(input_string, return_string=False, stem=False):\n",
    "    text = nlp(u'' + input_string)\n",
    "    if stem == True:\n",
    "        text = [tok.lemma_ for tok in text if (tok.is_alpha and not tok.is_stop)]\n",
    "    else:\n",
    "        text = [tok.lower_ for tok in text if (tok.is_alpha and not tok.is_stop)]\n",
    "    if return_string == True:\n",
    "        return \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make this statement true to run from scratch [It takes time to process the text]\n",
    "if 1 == 0:\n",
    "    wordlist = []\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        wordlist.append(process_text(df['review'].iloc[i]))\n",
    "        \n",
    "    with open('vocabulary.txt', 'wb') as vocabulary:\n",
    "        pickle.dump(wordlist, vocabulary)\n",
    "    vocabulary.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load vocabulary\n",
    "wordlist = []\n",
    "with open('vocabulary.txt', 'rb') as vocabulary:\n",
    "    wordlist = pickle.load(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens - 5464\n"
     ]
    }
   ],
   "source": [
    "# Keeping track of frequency of a single token\n",
    "frequency = defaultdict(int)\n",
    "for text in wordlist:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "        \n",
    "# Apply Threshold to limit the vocabulary size, discarding the tokens which appeard number of times below the threshold limit \n",
    "FREQ_THRESHOLD = 5\n",
    "\n",
    "thresholded_wordlist =  [[token for token in text if frequency[token] > FREQ_THRESHOLD]\n",
    "          for text in wordlist]\n",
    "\n",
    "# Create Dictionary based on the word list\n",
    "dictionary = Dictionary(thresholded_wordlist)\n",
    "\n",
    "# Number of tokens\n",
    "print(\"Number of Tokens - {}\".format(len(dictionary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(5464 unique tokens: ['able', 'admit', 'ben', 'character', 'comedy']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://i.imgur.com/f1uzTDZ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "* From the screenshot you can see the implementation of word-cooccurance matrix, based on the tokens from the dictionary, build a word-cooccurance matrix yourself which is $X$. Documentation of gensim [https://radimrehurek.com/gensim/corpora/dictionary.html]\n",
    "* Apply SVD on $X$\n",
    "* Reduce Dimension \n",
    "\n",
    "![dimen_reduc](https://i.imgur.com/lezB870.png)\n",
    "\n",
    "* Here Richard is taking only top two dimensions of the vector $U$, recommended size is *50* for now.\n",
    "\n",
    "![dimen_reduc_u](https://i.imgur.com/TA2Bmsq.png)\n",
    "\n",
    "* Now we can get a fixed size vector for each word. \n",
    "\n",
    "* Try to plot something similar based on the given dataset. In class we will try to implement a logistic regression classifier that can classify positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_d=len(dictionary)\n",
    "X=np.zeros((len_d,len_d))\n",
    "la=np.linalg\n",
    "for sen in thresholded_wordlist:\n",
    "    \n",
    "    idx=dictionary.doc2idx(sen)\n",
    "    for i in idx:\n",
    "        for j in idx:\n",
    "            X[i,j]=X[i,j]+1\n",
    "U,s,Vh = la.svd(X, full_matrices=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5464, 5464)\n"
     ]
    }
   ],
   "source": [
    "print(U.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5464,)\n"
     ]
    }
   ],
   "source": [
    "print(s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5464, 5464)\n"
     ]
    }
   ],
   "source": [
    "print(Vh.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range((len_d)):\n",
    "    plt.text(U_new[i,0], U_new[i,1], dictionary[i])\n",
    "    \n",
    "\n",
    "    #how to plot this "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
